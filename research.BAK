<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="ja" lang="ja">
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-F1TZMY54WS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-F1TZMY54WS');
</script>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Style-Type" content="text/css" />
<meta http-equiv="Content-Script-Type" content="text/javascript" />
<meta http-equiv="imagetoolbar" content="no" />
<meta name="description" content="" />
<meta name="keywords" content="" />
<link rel="stylesheet" href="css/common.css" type="text/css" />
<script type="text/javascript" src="js/jquery.js"></script>
<script type="text/javascript" src="js/common.js"></script>
<title>Research</title>
</head>
<body>
<div id="top">
   <div id="header">
      <h1>Research</h1>
      <p>
      I am currently working on developing reinforcement learning methods. Also, I have been working on research for spoken dialogue systems. 
      </p>
      <div id="navi">
         <ul>
         </ul>
      </div><!-- /#navi-->
   </div><!-- /#header-->
   <div id="menu">
      <ul>
      <li><a href="index.html">PROFILE</a></li>
      <li><a href="#" class="on">RESEARCH</a></li>
      <li><a href="publications.html">PUBLICATIONS</a></li>
      </ul>
   </div><!-- /#menu-->
   <div id="contents">

      <h2>Reinforcement Learning</h2>

      <h3>Efficient method for estimating the influence of experiences on RL agnet's performance (2023 -- , NEC and AIST)</h3>

<iframe src="https://drive.google.com/file/d/1VnHMmhptasUn0ohVZjRyI9goJYFa1pMR/preview" width="640" height="480" allow="autoplay"></iframe>

         <iframe src="https://drive.google.com/file/d/1VnHMmhptasUn0ohVZjRyI9goJYFa1pMR/preview" width="640" height="480" allow="autoplay"></iframe>
         <!-- <iframe src="https://drive.google.com/file/d/1jtQP0VVuJzPX778DtvgvSDE-mCD0wrbv/preview" width="640" height="480"></iframe> -->
        <li>
         <u>Takuya Hiraoka</u>, Guanquan Wang, Takashi Onishi, Yoshimasa Tsuruoka.<br>
         <a href="https://arxiv.org/abs/2405.14629"><strong>
          Which Experiences Are Influential for RL Agents? Efficiently Estimating The Influence of Experiences
         </strong></a><br>
	 arXiv:2405.14629, 2024<br>
         <a href="https://drive.google.com/file/d/1t7iYHA8RE-JOmfgEL869-JpZenfLgV5D/view?usp=sharing">demo video 1</a>, 
         <a href="https://drive.google.com/file/d/1VnHMmhptasUn0ohVZjRyI9goJYFa1pMR/view?usp=sharing">demo video 2</a><br>
        </li><br>


      <h3>Simple but Efficient Reinforcement Learning Method for Sparse-Reward Goal-Conditioned tasks (2023, NEC and AIST)</h3>
         <iframe src="https://drive.google.com/file/d/1UHd7JVPCwFLNFhy1QcycQfwU_nll_yII/preview" width="640" height="480"></iframe>
        <li>
         <u>Takuya Hiraoka</u>.<br>
         <a href="https://arxiv.org/abs/2312.05787"><strong>
          Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization
         </strong></a><br>
	 arXiv:2312.05787, 2023<br>
         <a href="https://github.com/TakuyaHiraoka/Efficient-SRGC-RL-with-a-High-RR-and-Regularization">source code</a><br>
        </li><br>


      <h3>Very Simple but Doubly Efficient Reinforcement Learning Method (2021, NEC and AIST)</h3>
        <img src="./images/hiraoka22iclr.jpg" width="624" height="366" style="border:solid 2px #000000">
        <li>
         <u>Takuya Hiraoka</u>, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, Yoshimasa Tsuruoka.<br>
         <a href="https://openreview.net/forum?id=xCVJMsPv3RT"><strong>
          Dropout Q-Functions for Doubly Efficient Reinforcement Learning
         </strong></a><br>
	 The Tenth International Conference on Learning Representations (ICLR), April, 2022<br>
	 <a href="https://drive.google.com/file/d/1_JSuwlUsMjzo6zRaAIcXXj3__AmOvu2t/view?usp=sharing">poster</a>, 
	 <a href="https://drive.google.com/file/d/1ecq9SQ2KSNpfeblCkr6TYPz5gRk_Y4S8/view?usp=sharing">slide</a>, 
         <a href="https://arxiv.org/abs/2110.02034">arXiv version</a>, 
         <a href="https://github.com/TakuyaHiraoka/Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning">source code</a><br>
        </li><br>

      <h3>Model Based Meta Reinforcement Learning (2020, NEC, AIST and RIKEN)</h3>
         <iframe src="https://drive.google.com/file/d/1DRA-pmIWnHGNv5G_gFrml8YzKCtMcGnu/preview" width="640" height="480"></iframe>
        <li>
         <u>Takuya Hiraoka</u>, Takahisa Imagawa, Voot Tangkaratt, Takayuki Osa, Takashi Onishi, Yoshimasa Tsuruoka.<br>
         <a href="http://www.acml-conf.org/2021/conference/accepted-papers/135/"><strong>
          Meta-Model-Based Meta-Policy Optimization
         </strong></a><br>
	 The 13th Asian Conference on Machine Learning (ACML), November, 2021<br>
	 <a href="https://drive.google.com/file/d/1ueznSgtqgtln_RwUmLdO0F9dL-niQciU/view?usp=sharing">poster</a>, 
	 <a href="https://slideslive.com/38970396/metamodelbased-metapolicy-optimization?ref=account-folder-92357-folders">video presentation</a>, 
	 <a href="https://drive.google.com/file/d/1DRA-pmIWnHGNv5G_gFrml8YzKCtMcGnu/view?usp=sharing">demo video</a>, 
         <a href="https://arxiv.org/abs/2006.02608">arXiv version</a>, 
         <a href="https://github.com/TakuyaHiraoka/Meta-Model-Based-Meta-Policy-Optimization">source code</a><br>
        </li><br>

      <h3>Robust Hierarchical Reinforcement Learning (2019, NEC and AIST)</h3>
         <iframe src="https://drive.google.com/file/d/1xXgSeEa_nNG397ZkIayk3CwYPy_BPy8X/preview" width="640" height="480"></iframe>
        <li>
         <u>Takuya Hiraoka</u>, Takahisa Imagawa, Tatsuya Mori, Takashi Onishi, Yoshimasa Tsuruoka.<br>
         <a href="https://papers.nips.cc/paper/8530-learning-robust-options-by-conditional-value-at-risk-optimization"><strong>Learning Robust Options by Conditional Value at Risk Optimization</strong></a><br>
	 Thirty-third Conference on Neural Information Processing System (NeurIPS), December 2019<br>
        </li><br>

      <h2>Dialogue Systems</h2>
      <h3>Inquiry Dialogue System (2018, NEC)</h3>
         <iframe src="https://drive.google.com/file/d/1BeVDX3uDd_FQ1vqp0dB2F4oVhSphlGGc/preview" width="640" height="480"></iframe>
         <li>
         <u>Takuya Hiraoka</u>, Shota Motoura, Kunihiko Sadamasa, 
         <a href="./papers/hiraoka18iwsds.pdf"><strong>Detecticon: A Prototype Inquiry Dialog System</strong></a>, 
         9th International Workshop on Spoken Dialog Systems (IWSDS). May 2018<br> 
        </li><br>

      <h3>Persuasive Dialogue System (~2016, NAIST)</h3>
        <iframe src="https://drive.google.com/file/d/1z1jvORPEQpfwMEGdcBDDe7OZZgN4pbKS/preview" width="640" height="480"></iframe> 
        <li>
         <u>Takuya Hiraoka</u>, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura, 
         <a href="./papers/hiraoka14coling.pdf"><strong>Reinforcement Learning of Cooperative Persuasive Dialogue Policies using Framing</strong></a>, 
	 The 25th International Conference on Computational Linguistics (COLING), August 2014<br>
        </li>
        <br>
        <iframe width="640" height="480" src="https://www.youtube.com/embed/YklLhJtVdvg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <li>
         <u>Takuya Hiraoka</u>, Yuki Yamauchi, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura, 
         <a href="./papers/hiraoka13asru.pdf"><strong>Dialogue Management for Leading the Conversation in Persuasive Dialogue Systems</strong></a>, 
         Automatic Speech Recognition and Understanding Workshop (ASRU), December 2013<br>
        </li><br>

      <h3>Example-based Dialogue System (2016, NAIST)</h3>
        <img src="./images/hiraoka16iwsds.jpg" width="624" height="366" style="border:solid 2px #000000">
        <li>
         <u>Takuya Hiraoka</u>, Graham Neubig, Koichiro Yoshino, Tomoki Toda, Satoshi Nakamura, 
         <a href="./papers/hiraoka16iwsds.pdf"><strong>
          Active Learning for Example-based Dialog Systems
         </strong></a>, 
	 7th International Workshop on Spoken Dialog Systems (IWSDS). January 2016, 
         <a href="https://github.com/TakuyaHiraoka/Active-Learning-for-Example-based-Dialog-Systems">Source code</a>
        </li><br>

      <h3>Negotiation Dialogue System (2015, NAIST and USC)</h3>
        <img src="./images/hiraoka15sigdial.jpg" width="659" height="332" style="border:solid 2px #000000">
        <li>
         <u>Takuya Hiraoka</u>, Kallirroi Georgila, Elnaz Nouri, David Traum, Satoshi Nakamura,<br> 
         <a href="./papers/hiraoka15sigdial.pdf"><strong> 
          Reinforcement Learning in Multi-Party Trading Dialog
         </strong></a>, 
	 The 16th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL). September 2015, 
         <a href="https://github.com/TakuyaHiraoka/Reinforcement-Learning-in-Multi-Party-Trading-Dialog">Source code</a>, 
         <a href="http://www.superlectures.com/sigdial2015/oral-session-2-discourse-strategy">video recordings and slides</a>
        </li><br>

    <div>
   </div><!-- /#pageTop-->
   <div id="footer">
            <div class="copyright">Copyright &copy; 2014 Takuya Hiraoka All Rights Reserved.</div>
   </div><!-- /#footer-->
</div><!-- /#top-->
</body>
</html>