<!doctype html>
<html lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-F1TZMY54WS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-F1TZMY54WS');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="" />
<meta name="keywords" content="" />
<link rel="stylesheet" href="css/common.css" type="text/css" />
<style type="text/css">
#contents iframe { width: 100%; display: block; }
</style>
<script type="text/javascript" src="js/common.js"></script>
<title>Research</title>
</head>
<body>
<div id="top">
   <div id="header">
      <h1>Research</h1>
      <p>
      I am currently working on developing reinforcement learning methods. Also, I have been working on research for spoken dialogue systems. 
      </p>
      <div id="navi">
         <ul>
         </ul>
      </div><!-- /#navi-->
   </div><!-- /#header-->
   <div id="menu">
      <ul>
      <li><a href="index.html">PROFILE</a></li>
      <li><a href="#" class="on">RESEARCH</a></li>
      <li><a href="publications.html">PUBLICATIONS</a></li>
      </ul>
   </div><!-- /#menu-->
   <div id="contents">

      <h2>Reinforcement Learning</h2>

      <h3>Efficient method for estimating the influence of experiences on RL agent's performance (2023 ~ 2025, NEC and AIST)</h3>
         <iframe title="video" src="https://drive.google.com/file/d/1VnHMmhptasUn0ohVZjRyI9goJYFa1pMR/preview" width="640" height="480" allow="autoplay"></iframe>
         <!-- <iframe src="https://drive.google.com/file/d/1jtQP0VVuJzPX778DtvgvSDE-mCD0wrbv/preview" width="640" height="480"></iframe> -->
        <ul>
        <li>
         <u>Takuya Hiraoka</u>, Guanquan Wang, Takashi Onishi, Yoshimasa Tsuruoka.<br>
         <a href="https://openreview.net/forum?id=pUvF97zAu9"><strong>
          Which Experiences Are Influential for RL Agents? Efficiently Estimating The Influence of Experiences
         </strong></a><br>
	  Reinforcement Learning Conference (RLC) / Journal (RLJ), 2025<br>
         <a href="https://drive.google.com/file/d/1fqd5UPUNOQniG-CshmdFFPxEG9m7W4hS/view?usp=sharing">poster</a>, 
         <a href="https://drive.google.com/file/d/1JjOMvA-oF7bas2OJmO_en6mJAtNGoLjs/view?usp=sharing">slides</a>, 
         <a href="https://github.com/TakuyaHiraoka/Which-Experiences-Are-Influential-for-RL-Agents">source code</a>, 
         <a href="https://drive.google.com/file/d/1t7iYHA8RE-JOmfgEL869-JpZenfLgV5D/view?usp=sharing">demo video 1</a>, 
         <a href="https://drive.google.com/file/d/1VnHMmhptasUn0ohVZjRyI9goJYFa1pMR/view?usp=sharing">demo video 2</a>, 
         <a href="https://arxiv.org/abs/2405.14629">arXiv</a>, 
         <a href="https://rlj.cs.umass.edu/2025/papers/Paper4.html">RLJ version</a><br>
        </li></ul><br>

      <h3>Simple but Efficient Reinforcement Learning Method for Sparse-Reward Goal-Conditioned tasks (2023, NEC and AIST)</h3>
         <iframe title="video" src="https://drive.google.com/file/d/1UHd7JVPCwFLNFhy1QcycQfwU_nll_yII/preview" width="640" height="480" allow="autoplay"></iframe>
        <ul>
        <li>
         <u>Takuya Hiraoka</u>.<br>
         <a href="https://arxiv.org/abs/2312.05787"><strong>
          Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization
         </strong></a><br>
	 arXiv:2312.05787, 2023<br>
         <a href="https://github.com/TakuyaHiraoka/Efficient-SRGC-RL-with-a-High-RR-and-Regularization">source code</a><br>
        </li></ul><br>


      <h3>Very Simple but Doubly Efficient Reinforcement Learning Method (2021, NEC and AIST)</h3>
        <img src="./images/hiraoka22iclr.jpg" width="624" height="366" style="border:solid 2px #000000" alt="Dropout Q-Functions for Doubly Efficient Reinforcement Learning">
        <ul>
        <li>
         <u>Takuya Hiraoka</u>, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, Yoshimasa Tsuruoka.<br>
         <a href="https://openreview.net/forum?id=xCVJMsPv3RT"><strong>
          Dropout Q-Functions for Doubly Efficient Reinforcement Learning
         </strong></a><br>
	 The Tenth International Conference on Learning Representations (ICLR), April, 2022<br>
	 <a href="https://drive.google.com/file/d/1_JSuwlUsMjzo6zRaAIcXXj3__AmOvu2t/view?usp=sharing">poster</a>, 
	 <a href="https://drive.google.com/file/d/1ecq9SQ2KSNpfeblCkr6TYPz5gRk_Y4S8/view?usp=sharing">slide</a>, 
         <a href="https://arxiv.org/abs/2110.02034">arXiv version</a>, 
         <a href="https://github.com/TakuyaHiraoka/Dropout-Q-Functions-for-Doubly-Efficient-Reinforcement-Learning">source code</a><br>
        </li></ul><br>

      <h3>Model Based Meta Reinforcement Learning (2020, NEC, AIST and RIKEN)</h3>
         <iframe title="video" src="https://drive.google.com/file/d/1DRA-pmIWnHGNv5G_gFrml8YzKCtMcGnu/preview" width="640" height="480" allow="autoplay"></iframe>
        <ul>
        <li>
         <u>Takuya Hiraoka</u>, Takahisa Imagawa, Voot Tangkaratt, Takayuki Osa, Takashi Onishi, Yoshimasa Tsuruoka.<br>
         <a href="http://www.acml-conf.org/2021/conference/accepted-papers/135/"><strong>
          Meta-Model-Based Meta-Policy Optimization
         </strong></a><br>
	 The 13th Asian Conference on Machine Learning (ACML), November, 2021<br>
	 <a href="https://drive.google.com/file/d/1ueznSgtqgtln_RwUmLdO0F9dL-niQciU/view?usp=sharing">poster</a>, 
	 <a href="https://slideslive.com/38970396/metamodelbased-metapolicy-optimization?ref=account-folder-92357-folders">video presentation</a>, 
	 <a href="https://drive.google.com/file/d/1DRA-pmIWnHGNv5G_gFrml8YzKCtMcGnu/view?usp=sharing">demo video</a>, 
         <a href="https://arxiv.org/abs/2006.02608">arXiv version</a>, 
         <a href="https://github.com/TakuyaHiraoka/Meta-Model-Based-Meta-Policy-Optimization">source code</a><br>
        </li></ul><br>

      <h3>Robust Hierarchical Reinforcement Learning (2019, NEC and AIST)</h3>
         <iframe title="video" src="https://drive.google.com/file/d/1xXgSeEa_nNG397ZkIayk3CwYPy_BPy8X/preview" width="640" height="480" allow="autoplay"></iframe>
        <ul>
        <li>
         <u>Takuya Hiraoka</u>, Takahisa Imagawa, Tatsuya Mori, Takashi Onishi, Yoshimasa Tsuruoka.<br>
         <a href="https://papers.nips.cc/paper/8530-learning-robust-options-by-conditional-value-at-risk-optimization"><strong>Learning Robust Options by Conditional Value at Risk Optimization</strong></a><br>
	 Thirty-third Conference on Neural Information Processing System (NeurIPS), December 2019<br>
        </li></ul><br>

      <h2>Dialogue Systems</h2>
      <h3>Inquiry Dialogue System (2018, NEC)</h3>
         <iframe title="video" src="https://drive.google.com/file/d/1BeVDX3uDd_FQ1vqp0dB2F4oVhSphlGGc/preview" width="640" height="480" allow="autoplay"></iframe>
         <ul>
         <li>
         <u>Takuya Hiraoka</u>, Shota Motoura, Kunihiko Sadamasa, 
         <a href="./papers/hiraoka18iwsds.pdf"><strong>Detecticon: A Prototype Inquiry Dialog System</strong></a>, 
         9th International Workshop on Spoken Dialog Systems (IWSDS). May 2018<br> 
        </li></ul><br>

      <h3>Persuasive Dialogue System (~2016, NAIST)</h3>
        <iframe title="video" src="https://drive.google.com/file/d/1z1jvORPEQpfwMEGdcBDDe7OZZgN4pbKS/preview" width="640" height="480" allow="autoplay"></iframe> 
        <ul>
        <li>
         <u>Takuya Hiraoka</u>, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura, 
         <a href="./papers/hiraoka14coling.pdf"><strong>Reinforcement Learning of Cooperative Persuasive Dialogue Policies using Framing</strong></a>, 
	 The 25th International Conference on Computational Linguistics (COLING), August 2014<br>
        </li>
        </ul>
        <br>
        <iframe title="video" width="640" height="480" src="https://www.youtube.com/embed/YklLhJtVdvg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <ul>
        <li>
         <u>Takuya Hiraoka</u>, Yuki Yamauchi, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura, 
         <a href="./papers/hiraoka13asru.pdf"><strong>Dialogue Management for Leading the Conversation in Persuasive Dialogue Systems</strong></a>, 
         Automatic Speech Recognition and Understanding Workshop (ASRU), December 2013<br>
        </li></ul><br>

      <h3>Example-based Dialogue System (2016, NAIST)</h3>
        <img src="./images/hiraoka16iwsds.jpg" width="624" height="366" style="border:solid 2px #000000" alt="Active Learning for Example-based Dialog Systems">
        <ul>
        <li>
         <u>Takuya Hiraoka</u>, Graham Neubig, Koichiro Yoshino, Tomoki Toda, Satoshi Nakamura, 
         <a href="./papers/hiraoka16iwsds.pdf"><strong>
          Active Learning for Example-based Dialog Systems
         </strong></a>, 
	 7th International Workshop on Spoken Dialog Systems (IWSDS). January 2016, 
         <a href="https://github.com/TakuyaHiraoka/Active-Learning-for-Example-based-Dialog-Systems">Source code</a>
        </li></ul><br>

      <h3>Negotiation Dialogue System (2015, NAIST and USC)</h3>
        <img src="./images/hiraoka15sigdial.jpg" width="659" height="332" style="border:solid 2px #000000" alt="Reinforcement Learning in Multi-Party Trading Dialog">
        <ul>
        <li>
         <u>Takuya Hiraoka</u>, Kallirroi Georgila, Elnaz Nouri, David Traum, Satoshi Nakamura,<br> 
         <a href="./papers/hiraoka15sigdial.pdf"><strong> 
          Reinforcement Learning in Multi-Party Trading Dialog
         </strong></a>, 
	 The 16th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL). September 2015, 
         <a href="https://github.com/TakuyaHiraoka/Reinforcement-Learning-in-Multi-Party-Trading-Dialog">Source code</a>, 
         <a href="http://www.superlectures.com/sigdial2015/oral-session-2-discourse-strategy">video recordings and slides</a>
        </li></ul><br>

   </div><!-- /#contents-->
    <div id="pageTop">
   </div><!-- /#pageTop-->
   <div id="footer">
            <div class="copyright">Copyright &copy; 2014 Takuya Hiraoka All Rights Reserved.</div>
   </div><!-- /#footer-->
</div><!-- /#top-->
</body>
</html>